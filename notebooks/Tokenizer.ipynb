{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51266146-d6bf-4e23-bd4c-1215eb533ee6",
   "metadata": {},
   "source": [
    "# Tokenizer Module Specification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75579247-b691-493f-808e-19fc989c70d6",
   "metadata": {},
   "source": [
    "#### We introduce a Tokenizer class engineered for robust lexical tokenization workflows. This class encapsulates both encode and decode methods, facilitating bidirectional transformation between raw text sequences and token ID representations. It maintains two core attributes: a vocab dictionary mapping string tokens to integer IDs, and a reverse mapping enabling lossless detokenization.\n",
    "\n",
    "#### Instantiation of the Tokenizer requires a pretrained vocabulary derived from the corpus \"The Verdict\", ensuring domain-adapted lexical coverage. The model includes explicit support for special tokens:\n",
    "###### <|unk|> — a reserved identifier for out-of-vocabulary (OOV) tokens.\n",
    "###### <|endoftext|> — a sentinel token indicating text sequence termination, particularly useful in multi-source input streams for boundary demarcation and coherent downstream processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282e2e23-2646-4713-9d7b-7e9768e6027c",
   "metadata": {},
   "source": [
    "#### To create vocabulary we'll use the book \"The Verdict\" by Edith Wharton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "997aa4c5-d0e2-4740-b8b5-2dc8bd861a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of character: 20479\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "    \n",
    "print(\"Total number of character:\", len(raw_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb8ede3-fe7f-49ad-82e1-b375273914e1",
   "metadata": {},
   "source": [
    "#### Use re(Regex) to format data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b1f5e186-4247-4455-8cc6-ccac690034e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4690\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "\n",
    "print(len(preprocessed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0201e333-9851-4e85-9011-bb7bc5983162",
   "metadata": {},
   "source": [
    "#### Constructing The vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "07a49381-327b-449a-b7df-0fd8919aee6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1132\n",
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "('your', 1128)\n",
      "('yourself', 1129)\n",
      "('<|endoftext|>', 1130)\n",
      "('<|unk|>', 1131)\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(preprocessed))\n",
    "all_words.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "\n",
    "vocab_size = len(all_words)\n",
    "vocab = {token:integer for integer,token in enumerate(all_words)}\n",
    "\n",
    "print(vocab_size)\n",
    "\n",
    "for i, item in enumerate(vocab.items()):\n",
    "    if i < 5 or i>=1128:\n",
    "        print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86694dd-d6b8-42f6-9aa5-c216bd22915d",
   "metadata": {},
   "source": [
    "#### Tokenizer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aeb1aabd-fa50-4d85-8671-2cd136d49c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = { i:s for s,i in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed = [\n",
    "            item if item in self.str_to_int \n",
    "            else \"<|unk|>\" for item in preprocessed\n",
    "        ]\n",
    "\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "        \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a23e70-b5ff-48b3-a937-6cad36fc93ec",
   "metadata": {},
   "source": [
    "#### Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6c6234c3-4373-4525-9284-f073ef81e0a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, my friend how do you do? <|endoftext|> In the sunlit terraces of the Gittusburgs.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(vocab)\n",
    "\n",
    "text1 = \"Hello, my friend how do you do?\"\n",
    "text2 = \"In the sunlit terraces of the Gittusburgs.\"\n",
    "\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "46b9faaa-607c-4249-83e4-96e2b30af091",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e34a356a-947a-4be2-91d8-3ec97fd97ac6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|unk|>, my friend how do you do? <|endoftext|> In the sunlit terraces of the <|unk|>.'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
